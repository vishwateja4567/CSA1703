import math
from collections import Counter
print("192311339-M.Viswa teja")
# Sample dataset (Play Tennis)
dataset = [
    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'Play': 'No'},
    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'Play': 'No'},
    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'Play': 'Yes'},
    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'Play': 'Yes'},
    {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'Play': 'Yes'},
    {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'Play': 'No'},
    {'Outlook': 'Overcast', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'Play': 'Yes'},
    {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'Play': 'No'},
    {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'Play': 'Yes'},
    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Wind': 'Weak', 'Play': 'Yes'},
    {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Wind': 'Strong', 'Play': 'Yes'},
    {'Outlook': 'Overcast', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Strong', 'Play': 'Yes'},
    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'Normal', 'Wind': 'Weak', 'Play': 'Yes'},
    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Strong', 'Play': 'No'},
]

features = ['Outlook', 'Temperature', 'Humidity', 'Wind']

# Calculate entropy
def entropy(data):
    label_counts = Counter(row['Play'] for row in data)
    total = len(data)
    return -sum((count / total) * math.log2(count / total) for count in label_counts.values())

# Split dataset based on a feature
def split_data(data, feature):
    values = set(row[feature] for row in data)
    return {v: [row for row in data if row[feature] == v] for v in values}

# Information gain
def info_gain(data, feature):
    total_entropy = entropy(data)
    subsets = split_data(data, feature)
    subset_entropy = sum((len(subset) / len(data)) * entropy(subset) for subset in subsets.values())
    return total_entropy - subset_entropy

# Build the tree using ID3
def build_tree(data, features):
    labels = [row['Play'] for row in data]
    if labels.count(labels[0]) == len(labels):
        return labels[0]
    if not features:
        return Counter(labels).most_common(1)[0][0]

    best_feature = max(features, key=lambda f: info_gain(data, f))
    tree = {best_feature: {}}
    for value, subset in split_data(data, best_feature).items():
        subtree = build_tree(subset, [f for f in features if f != best_feature])
        tree[best_feature][value] = subtree
    return tree

# Print the tree
def print_tree(tree, indent=''):
    if isinstance(tree, dict):
        for key, subtree in tree.items():
            print(indent + str(key))
            for val, branch in subtree.items():
                print(indent + f'  {val} ->', end=' ')
                print_tree(branch, indent + '    ')
    else:
        print(tree)

# Run the program
decision_tree = build_tree(dataset, features)
print("Decision Tree:")
print_tree(decision_tree)
